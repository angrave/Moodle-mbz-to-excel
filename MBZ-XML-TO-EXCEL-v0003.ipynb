{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBZ-XML-TO-EXCEL\n",
    "\n",
    "\n",
    "Lawrence Angrave. First pubished version May 22, 2019.  This is version 0.0003 (July 8, 2019)\n",
    "\n",
    "Licensed under the NCSA Open source license\n",
    "Copyright (c) 2019 Lawrence Angrave\n",
    "All rights reserved.\n",
    "\n",
    "Developed by: Lawrence Angrave\n",
    " \n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "   Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\n",
    "   Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\n",
    "   Neither the names of Lawrence Angrave, University of Illinois nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. \n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE. \n",
    "\n",
    "# Citations\n",
    "\n",
    "In a presentation, report or paper please recognise and acknowledge the the use of this software.\n",
    "Please contact angrave@illinois.edu for a Bibliography citation. For presentations, the following is sufficient\n",
    "\n",
    "MBZ-XML-TO-EXCEL (https://github.com/angrave/Moodle-mbz-to-excel) by Lawrence Angrave.\n",
    "MBZ-XML-TO-EXCEL is an iLearn project, supported by an Institute of Education Sciences Award R305A180211\n",
    "\n",
    "If also using Geo-IP data, please cite IP2Location. For example,\n",
    "\"This report uses geo-ip location data from IP2Location.com\"\n",
    "\n",
    "# Known Limitations and Issues\n",
    "\n",
    "The assessment sheet (generated from workshop.xml) may generate URLs that are longer than 255 characters, \n",
    "the largested supported by Excel. These very long URLs will be excluded\n",
    "\n",
    "No verification of the data has been performed. \n",
    "\n",
    "It is unknown if the inferred timestamps based on the Unix Epoch timestamp require a timezone adjustment.\n",
    "\n",
    "# Requirements\n",
    "\n",
    "This project uses Python3, Jupiter notebooks and Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import tarfile\n",
    "import tempfile\n",
    "import base64\n",
    "# geoip support\n",
    "import bisect\n",
    "import ipaddress\n",
    "# timestamp support\n",
    "from datetime import datetime\n",
    "# Extract text from html messages\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid\n",
    "import traceback\n",
    "\n",
    "import xlsxwriter\n",
    "excelengine = 'xlsxwriter' \n",
    "# 'xlsxwriter'(recommended though did not improve the write speed)\n",
    "# io.excel.xlsx.writer' (default, allegedly slow),\n",
    "# 'pyexcelerate' (untested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load optional GeoIP support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_all_colnames = ['geoip_ipfrom'\n",
    ",'geoip_ipto'\n",
    ",'geoip_country_code'\n",
    ",'geoip_country_name'\n",
    ",'geoip_region_name'\n",
    ",'geoip_city_name'\n",
    ",'geoip_latitude'\n",
    ",'geoip_longitude'\n",
    ",'geoip_zip_code'\n",
    ",'geoip_time_zone']\n",
    "\n",
    "geoip_geo_columns = geoip_all_colnames[2:]\n",
    "\n",
    "geoipv4_csv = os.path.join('geoip','IP2LOCATION-LITE-DB11.CSV')\n",
    "\n",
    "if os.path.exists(geoipv4_csv):\n",
    "    print(\"Reading geoip csv\",geoipv4_csv)\n",
    "    geoipv4_df = pd.read_csv(geoipv4_csv, names= geoip_all_colnames)\n",
    "    geoipv4_ipvalues = geoipv4_df['geoip_ipfrom'].values\n",
    "    # bisect searching assumes geoipv4_ipvalues are in increasing order, \n",
    "else:\n",
    "    geoipv4_df = None\n",
    "    geoipv4_ipvalues = None\n",
    "    print(\"No GeoIP csv data at \",geoipv4_csv)\n",
    "    print(\"IP addresses will not be converted into geographic locations\")\n",
    "    print(\"Free Geo-IP data can be downloaded from IP2LOCATION.com\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Extract XMLs from mbz file and create hundreds of Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each file can generate a list of tables (dataframes)\n",
    "# Recursively process each element. \n",
    "# For each non-leaf element we build an ordered dictionary of key-value pairs and attach this to an array for the particular element name\n",
    "# <foo id='1' j='a'> becomes data['foo'] = [ {'id':'1', j:'a'} ]\n",
    "# The exception is for leaf elements (no-child elements) in the form e.g. <blah>123</blah>\n",
    "# We treat these equivalently to attributes on the surrounding (parent) xml element\n",
    "# <foo id='1'><blah>123</blah></foo> becomes data['foo'] = [ {'id':'1', 'blah':'123'} ]\n",
    "# and no data['blah'] is created\n",
    "\n",
    "def process_element(data,dest_basedir, tablename_list, context, e):\n",
    "    has_no_children = len(e.getchildren()) == 0\n",
    "    has_no_attribs = len(e.attrib.keys()) == 0\n",
    "    text = e.text\n",
    "        \n",
    "    has_text = text is not None\n",
    "    if has_text:\n",
    "        text = text.strip()\n",
    "        has_text = len(text) > 0\n",
    "        \n",
    "    # Is this a leaf element e.g. <blah>123</blah>\n",
    "    # For the datasets we care about, leaves should not be tables; we only want their value   \n",
    "    ignore_attribs_on_leaves = True\n",
    "    \n",
    "    # This could be refactored to return a dictionary, so multiple attributes can be attached to the parent\n",
    "    if has_no_children and (has_no_attribs or ignore_attribs_on_leaves):\n",
    "        if not has_no_attribs: \n",
    "            print()\n",
    "            print(\"Warning: Ignoring attributes on leaf element:\" + e.tag+ \":\"+ str(e.attrib))\n",
    "            print()\n",
    "        return [e.tag,e.text] # Early return, attach the value to the parent (using the tag as the attribute name)\n",
    "    \n",
    "    table_name = e.tag\n",
    "    if table_name not in data:\n",
    "        tablename_list.append(table_name)\n",
    "        data[table_name] = []\n",
    "        \n",
    "    key_value_pairs = OrderedDict()\n",
    "    if context:\n",
    "        key_value_pairs['PARENT_XML_TAG'] = context[0]\n",
    "        key_value_pairs['PARENT_XML_INDEX'] = context[1]\n",
    "    \n",
    "    # For consistency these two lines need to be together (i.e. no recursion between them)\n",
    "    data[table_name].append(key_value_pairs)\n",
    "    child_context = [table_name, len(data[table_name])-1]\n",
    "\n",
    "    for key in sorted(e.attrib.keys()):\n",
    "        key_value_pairs[key] = e.attrib[key]\n",
    "\n",
    "    for child in e.getchildren():\n",
    "        # Could refactor here to use dictionary to enable multiple key-values from a discarded leaf\n",
    "        key,value = process_element(data,dest_basedir, tablename_list, child_context, child)\n",
    "        if value:\n",
    "            if key in key_value_pairs:\n",
    "                key_value_pairs[key] += ',' + str(value)\n",
    "            else:\n",
    "                key_value_pairs[key] = str(value)\n",
    "\n",
    "    \n",
    "    if has_text:\n",
    "        key_value_pairs['TEXT'] = e.text # If at least some non-whitespace text, then use original text\n",
    "    \n",
    "    return [e.tag,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablename_to_sheetname(elided_sheetnames, tablename):\n",
    "    sheetname = tablename\n",
    "    # Future: There may be characters that are invalid. If so, remove them here..\n",
    "\n",
    "    #Excel sheetnames are limited to 31 characters.\n",
    "    max_excel_sheetname_length = 31\n",
    "    if len(sheetname) <= max_excel_sheetname_length:\n",
    "        return sheetname\n",
    "    \n",
    "    sheetname = sheetname[0:5] + '...' + sheetname[-20:]\n",
    "    elided_sheetnames.append(sheetname)\n",
    "    if elided_sheetnames.count(sheetname)>1:\n",
    "        sheetname += str( elided_sheetnames.count(sheetname) + 1)\n",
    "    \n",
    "    return sheetname\n",
    "\n",
    "def decode_base64_to_latin1(encoded_val):\n",
    "    try:\n",
    "        return str(base64.b64decode(encoded_val) , 'latin-1')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"Not base64 latin1?\", e)\n",
    "        return '??Not-latin1 text'\n",
    "\n",
    "\n",
    "def decode_geoip(ip):\n",
    "    try:\n",
    "        ip = ip.strip()\n",
    "        if not ip or geoipv4_df is None:\n",
    "            return pd.Series(None, index=geoip_geo_columns)\n",
    "        \n",
    "        ipv4 = int(ipaddress.IPv4Address(ip))\n",
    "        index = bisect.bisect(geoipv4_ipvalues, ipv4) - 1\n",
    "        entry = geoipv4_df.iloc[index]\n",
    "        assert entry.geoip_ipfrom  <= ipv4 and entry.geoip_ipto  >= ipv4\n",
    "        return entry[2:] # [geoip_geo_columns] # Drop ip_from and ip_to\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"Bad ip?\",ip, e)\n",
    "        return pd.Series(None, index=geoip_geo_columns)\n",
    "\n",
    "def decode_unixtimestamp_to_UTC(seconds):\n",
    "    if seconds == '':\n",
    "        return ''\n",
    "    try:\n",
    "        return datetime.utcfromtimestamp(int(seconds)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"Bad unix timestamp?\", seconds , e)\n",
    "        return ''\n",
    "\n",
    "def decode_html_to_text(html):\n",
    "    if html is np.nan:\n",
    "        return ''\n",
    "    try:\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        return soup.get_text()\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print('Bad html?',html, e)\n",
    "        return '???'\n",
    "\n",
    "def userid_to_anonid(userid):\n",
    "    global anonid_df\n",
    "    if userid is np.nan or len(userid) == 0:\n",
    "        return ''\n",
    "\n",
    "    row = anonid_df[ anonid_df['userid'] == userid ]\n",
    "    if len( row ) == 1:\n",
    "        return row['anonid'].values[0]\n",
    "    \n",
    "    generate_missing_anonid = True\n",
    "    if generate_missing_anonid:    \n",
    "        result = uuid.uuid4().hex\n",
    "        anonid_df = anonid_df.append({ 'userid':userid, 'anonid':result}, ignore_index=True)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def to_dataframe(table_data):\n",
    "    df = pd.DataFrame(table_data)\n",
    "    # Moodle dumps use $@NULL@$ for nulls\n",
    "    df.replace('$@NULL@$','',inplace = True)\n",
    "    \n",
    "    # We found two base64 encoded columns in Moodle data-\n",
    "    for col in df.columns & ['other','configdata']:\n",
    "        df[ str(col) + '_base64'] = df[str(col)].map(decode_base64_to_latin1)\n",
    "    \n",
    "    for col in df.columns & ['timestart','timefinish','added','backup_date','original_course_startdate','original_course_enddate','timeadded','firstaccess','lastaccess','lastlogin','currentlogin','timecreated','timemodified','created','modified']:\n",
    "        df[ str(col) + '_utc'] = df[str(col)].map(decode_unixtimestamp_to_UTC)\n",
    "    \n",
    "    # Extract text from html content\n",
    "    for col in df.columns & ['message', 'description','commenttext','intro','conclusion','summary','feedbacktext','content','feedback','info', 'questiontext' , 'answertext']:\n",
    "        df[ str(col) + '_text'] = df[str(col)].map(decode_html_to_text)\n",
    "    \n",
    "    # Moodle data has 'ip' and 'lastip' that are ipv4 dotted\n",
    "    # Currently only ipv4 is implemented. geoipv4_df is None if the cvs file was not found\n",
    "\n",
    "    if geoipv4_df is None:\n",
    "        for col in df.columns & ['ip','lastip']:\n",
    "            df = df.join( df[str(col)].apply(decode_geoip) )\n",
    "\n",
    "    for col in df.columns & ['userid']:\n",
    "        df[ 'anonid' ] = df[str(col)].map(userid_to_anonid)\n",
    "        \n",
    "    # Can add more MOODLE PROCESSING HERE :-)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def write_excel_sheets(base_filename, excelwriter, data,tablename_list):   \n",
    "    elided_sheetnames = []\n",
    "    for tablename in tablename_list:\n",
    "        df = to_dataframe(data[tablename])\n",
    "        df.index.rename(tablename, inplace=True)\n",
    "        df.insert(0, 'SourceFile', base_filename,allow_duplicates=True)\n",
    "        df.insert(1, 'XMLTag', tablename,allow_duplicates=True)\n",
    "        \n",
    "        sheetname = tablename_to_sheetname(elided_sheetnames, tablename)\n",
    "        if sheetname != tablename:\n",
    "            print(\"Writing \"+ tablename + \" as sheet \"+ sheetname)\n",
    "        else:\n",
    "            print(\"Writing sheet \"+ sheetname)\n",
    "        \n",
    "        # Use the table name for the index, so the PARENT_XML_TAG column matches\n",
    "        df.to_excel(excelwriter, sheet_name=sheetname, index_label=tablename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_empty_tables(data,tablename_list):\n",
    "    nonempty_tables = []\n",
    "    for tablename in tablename_list:\n",
    "        table = data[tablename]\n",
    "        # print(tablename, len(table),'rows')\n",
    "        if len(table) == 0:\n",
    "            # print(\"Skipping empty table\",tablename)\n",
    "            continue\n",
    "            \n",
    "        include = False\n",
    "        for row in table:\n",
    "            if len(row) > 2: # Found more than just PARENT_XML_TAG and PARENT_XML_FILE\n",
    "                include = True\n",
    "                break\n",
    "        \n",
    "        if include:\n",
    "            # print(\"Including\",tablename)\n",
    "            nonempty_tables.append(tablename)\n",
    "        else:\n",
    "            # print(\"Skipping unnecessary table\",tablename)\n",
    "            pass\n",
    "\n",
    "    return nonempty_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(dest_basedir, relative_sub_dir, xml_filename):\n",
    "    print('process_one_file(\\''+dest_basedir+'\\',\\''+relative_sub_dir+'\\',\\''+xml_filename+'\\')')\n",
    "    #print(\"Reading XML \" + xml_filename)\n",
    "    xmlroot = ET.parse(xml_filename).getroot()\n",
    "\n",
    "    #print(\"Processing...\")\n",
    "    data = dict()\n",
    "    tablename_list = []\n",
    "    \n",
    "    process_element(data, dest_basedir ,tablename_list, None, xmlroot)\n",
    "    \n",
    "    nonempty_tables = discard_empty_tables(data,tablename_list)\n",
    "    \n",
    "    if len(nonempty_tables) == 0:\n",
    "        #print(\"no tables left to write\")\n",
    "        return\n",
    "    \n",
    "    # We use underscore to collate source subdirectories\n",
    "    basename = os.path.basename(xml_filename).replace('.xml','').replace('_','')\n",
    "    \n",
    "    use_sub_dirs = False\n",
    "    if use_sub_dirs:\n",
    "        output_dir = os.path.join(dest_basedir, relative_sub_dir)\n",
    "\n",
    "        if not os.path.exists(output_dir): \n",
    "            os.mkdirs(output_dir)\n",
    "\n",
    "        output_filename = os.path.join(output_dir,  basename + '.xlsx')\n",
    "    else:\n",
    "        sub = relative_sub_dir.replace(os.sep,'_').replace('.','')\n",
    "        if (len(sub) > 0) and sub[-1] != '_':\n",
    "            sub = sub + '_'\n",
    "        output_filename = os.path.join(dest_basedir,  sub +  basename + '.xlsx')\n",
    "    \n",
    "    print(\"** Writing \", output_filename)\n",
    "    if False: # For debugging\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(output_filename):\n",
    "        os.remove(output_filename)\n",
    "       \n",
    "    excelwriter = pd.ExcelWriter(output_filename, engine= excelengine)\n",
    "    try:\n",
    "        write_excel_sheets(xml_filename, excelwriter, data,nonempty_tables)\n",
    "        excelwriter.close()\n",
    "    except Exception as ex:\n",
    "        traceback.print_exc()\n",
    "        print(type(ex))\n",
    "        print(ex)\n",
    "        pass\n",
    "    finally:\n",
    "        \n",
    "        excelwriter = None\n",
    "    print()\n",
    "\n",
    "def process_directory(xml_basedir, out_basedir, relative_sub_dir='.'):\n",
    "    xml_dir = os.path.join(xml_basedir, relative_sub_dir)\n",
    "    file_list = sorted(os.listdir(xml_dir))\n",
    "    \n",
    "    for filename in file_list:\n",
    "        if filename.endswith('.xml'):\n",
    "            print(\"Processing\", filename)\n",
    "            process_one_file(out_basedir, relative_sub_dir, os.path.join(xml_dir,filename))\n",
    "    \n",
    "    if False: # For debugging\n",
    "        return # Skip recursion for testing\n",
    "    # Recurse\n",
    "    for filename in file_list:\n",
    "        candidate_sub_dir = os.path.join(relative_sub_dir, filename)\n",
    "        if os.path.isdir( os.path.join(xml_basedir, candidate_sub_dir)) :   \n",
    "            process_directory(xml_basedir, out_basedir, candidate_sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_files_in_tar(tar_file, extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "    extract_count = 0\n",
    "    for tarinfo in tar_file:\n",
    "        if os.path.splitext(tarinfo.name)[1] == \".xml\":\n",
    "            #print(extract_dir, tarinfo.name)\n",
    "            tar_file.extract( tarinfo, path = extract_dir)\n",
    "            extract_count = extract_count + 1\n",
    "    return extract_count\n",
    "            \n",
    "def archive_file_to_output_dir(archive_file):\n",
    "    return os.path.splitext(archive_file)[0] + '-out'\n",
    "\n",
    "def archive_file_to_xml_dir(archive_file):\n",
    "    return os.path.splitext(archive_file)[0] + '-xml'\n",
    "    \n",
    "def lazy_extract_mbz(archive_source_file,expanded_archive_directory,skip_expanding_if_xml_files_found):\n",
    "    has_xml_files = len( glob.glob( os.path.join(expanded_archive_directory,'*.xml') ) ) > 0\n",
    "    \n",
    "    if has_xml_files and skip_expanding_if_xml_files_found:\n",
    "        print(\"*** Reusing existing xml files in\", expanded_archive_directory)\n",
    "        return\n",
    "    \n",
    "    if os.path.isdir(expanded_archive_directory):\n",
    "        print(\"*** Deleting existing files in\", expanded_archive_directory)\n",
    "        raise \"test\"\n",
    "        shutil.rmtree(expanded_archive_directory)\n",
    "        \n",
    "    with tarfile.open(archive_source_file, mode='r|*') as tf:\n",
    "        print(\"*** Expanding\",archive_source_file, \"to\", expanded_archive_directory)\n",
    "        extract_count = extract_xml_files_in_tar(tf, expanded_archive_directory)\n",
    "        print('***',extract_count,' xml files extracted')\n",
    "    \n",
    "def process_xml_files(expanded_archive_directory,out_basedir):\n",
    "    global anonid_df\n",
    "    \n",
    "    print(\"*** Source xml directory :\", expanded_archive_directory)\n",
    "    print(\"*** Output directory:\", out_basedir)\n",
    "\n",
    "    if not os.path.isdir(out_basedir): \n",
    "        os.makedirs(out_basedir)\n",
    "\n",
    "    process_directory(expanded_archive_directory, out_basedir,'.')\n",
    "    \n",
    "    anonid_df.to_csv( os.path.join(out_basedir,'userids_anonids.csv'), index = None, header=True)\n",
    "    \n",
    "    print(\"*** Finished processing XML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Aggregate Excel documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 - Aggregate multiple xlsx that are split across multiple course sections into a single Excel file\n",
    "def create_aggregate_sections_map(xlsx_dir):\n",
    "    xlsx_files = sorted( glob.glob(os.path.join(xlsx_dir,'*.xlsx')) )\n",
    "    sections_map = dict()\n",
    "\n",
    "    for source_file in xlsx_files:\n",
    "        path = source_file.split(os.path.sep) # TODO os.path.sep\n",
    "        nameparts = path[-1].split('_')\n",
    "        target = nameparts[:]\n",
    "        subnumber = None\n",
    "        if len(nameparts)>3 and nameparts[-3].isdigit(): subnumber = -3 # probably unnecessary as _ are removed from basename\n",
    "        if len(nameparts)>2 and nameparts[-2].isdigit(): subnumber = -2\n",
    "        if not subnumber: continue\n",
    "\n",
    "        target[subnumber] = 'ALLSECTIONS'\n",
    "\n",
    "        key = (os.path.sep.join(path[:-1]))  + os.path.sep+ ( '_'.join(target))\n",
    "        if key not in sections_map.keys():\n",
    "            sections_map[key] = []\n",
    "        sections_map[key].append(source_file)\n",
    "    return sections_map\n",
    "    \n",
    "# Phase 3 - Aggregate over common objects\n",
    "def create_aggregate_common_objects_map(xlsx_dir):\n",
    "    xlsx_files = sorted(glob.glob(os.path.join(xlsx_dir,'*.xlsx')))\n",
    "\n",
    "    combined_map = dict()\n",
    "    # path/_activities_workshop_ALLSECTIONS_logstores.xlsx will map to key=logstores.xlsx\n",
    "    for source_file in xlsx_files:\n",
    "        path = source_file.split(os.path.sep) # TODO os.path.sep\n",
    "        nameparts = path[-1].split('_')\n",
    "        target = nameparts[-1]\n",
    "\n",
    "        if 'ALL_' == path[-1][:4]:\n",
    "            continue # Guard against restarts\n",
    "\n",
    "        key = (os.path.sep.join(path[:-1]))  + os.path.sep+ ('ALL_' + target)\n",
    "        if key not in combined_map.keys():\n",
    "            combined_map[key] = []\n",
    "        combined_map[key].append(source_file)\n",
    "\n",
    "    return combined_map   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebase_index=\n",
    "# True: indicies will be rebased and unique in Excel sheet \n",
    "# False: indicies scoped to original file and refer to original count in original xml document\n",
    "\n",
    "def aggregate_multiple_excel_files(source_filenames,rebase_index = True):\n",
    "    allsheets = OrderedDict()\n",
    "    \n",
    "    # !! Poor sort  - it assumes the integers are the same char length. Todo improve so that filename_5_ < filename_10_  \n",
    "    for filename in sorted(source_filenames):\n",
    "        print(filename)\n",
    "        xl = pd.ExcelFile(filename)\n",
    "        for sheet in xl.sheet_names:\n",
    "            \n",
    "            df = xl.parse(sheet)\n",
    "            if sheet not in allsheets.keys():\n",
    "                allsheets[sheet] = df\n",
    "            else:\n",
    "                if rebase_index:\n",
    "                    rebase = len(allsheets[sheet])\n",
    "                    if 'PARENT_XML_INDEX' in df.columns:\n",
    "                        df['PARENT_XML_INDEX'] = df['PARENT_XML_INDEX'] + rebase\n",
    "                    df.index = df.index + rebase\n",
    "                    \n",
    "                allsheets[sheet] = allsheets[sheet].append(df, sort = False)\n",
    "        xl.close()\n",
    "    return allsheets\n",
    "\n",
    "def write_aggregated_model(output_filename, allsheets):\n",
    "    print(\"Writing\",output_filename)\n",
    "    excelwriter = pd.ExcelWriter(output_filename, engine = excelengine)\n",
    "    try:\n",
    "        print(\"Sheets \", allsheets.keys())\n",
    "        for sheetname,df in allsheets.items():\n",
    "            df.to_excel(excelwriter, sheet_name=sheetname, index='INDEX')\n",
    "        excelwriter.close()\n",
    "    except Exception as ex:\n",
    "        print(type(ex))\n",
    "        print(ex)\n",
    "        pass\n",
    "    finally:\n",
    "        excelwriter.close()\n",
    "\n",
    "def move_old_files(xlsx_dir, map, subdirname):\n",
    "    xlsxpartsdir = os.path.join(xlsx_dir,subdirname)\n",
    "    if not os.path.isdir(xlsxpartsdir): \n",
    "        os.mkdir(xlsxpartsdir)\n",
    "\n",
    "    for targetfile,sources in map.items():\n",
    "        for file in sources:\n",
    "\n",
    "            dest=os.path.join(xlsxpartsdir, os.path.basename(file))\n",
    "            print(dest)\n",
    "            os.rename(file, dest)\n",
    "\n",
    "def aggreate_over_sections(xlsx_dir):\n",
    "    sections_map= create_aggregate_sections_map(xlsx_dir)\n",
    "\n",
    "    for targetfile,sources in sections_map.items():\n",
    "        allsheets = aggregate_multiple_excel_files(sources)\n",
    "        write_aggregated_model(targetfile , allsheets)\n",
    "\n",
    "    move_old_files(xlsx_dir, sections_map,'_EACH_SECTION_')\n",
    "\n",
    "def aggreate_over_common_objects(xlsx_dir):\n",
    "    combined_map = create_aggregate_common_objects_map(xlsx_dir)\n",
    "    \n",
    "    for targetfile,sources in combined_map.items():\n",
    "        allsheets = aggregate_multiple_excel_files(sources)\n",
    "        write_aggregated_model(targetfile , allsheets)\n",
    "        \n",
    "    move_old_files(xlsx_dir, combined_map,'_ALL_SECTIONS_')\n",
    "\n",
    "def create_column_metalist(xlsx_dir):\n",
    "    metalist = []\n",
    "\n",
    "    for filename in sorted(glob.glob(os.path.join(xlsx_dir,'*.xlsx'))):\n",
    "        print(filename)\n",
    "        xl = pd.ExcelFile(filename)\n",
    "        filename_local = os.path.basename(filename)\n",
    "\n",
    "        for sheet in xl.sheet_names:\n",
    "            #print(sheet)\n",
    "            df = xl.parse(sheet,nrows=1)\n",
    "            #print(df.columns)\n",
    "            for column_name in df.columns:\n",
    "                metalist.append([filename_local,sheet,column_name])\n",
    "        xl.close()\n",
    "\n",
    "    meta_df = pd.DataFrame(metalist, columns=['file','sheet','column'])\n",
    "\n",
    "    meta_filename = os.path.join(xlsx_dir,'__All_COLUMNS.csv')\n",
    "    meta_df.to_csv(meta_filename,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / settings here\n",
    "archive_source_file = None\n",
    "\n",
    "expanded_archive_directory = None \n",
    "\n",
    "skip_expanding_if_xml_files_found = True\n",
    "\n",
    "output_directory = None\n",
    "\n",
    "anonid_csv_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the above here with the path to your mbz file (or expanded contents)\n",
    "archive_source_file = os.path.join('..','example.mbz')\n",
    "# ... or use expanded_archive_directory to point to an mbz file that has already been expanded into XML files\n",
    "\n",
    "# Some typical numbers:\n",
    "# A 400 student 15 week course with  16 sections\n",
    "# Created a 4GB mbz which expanded to 367 MB of xml. (the non-xml files were not extracted)\n",
    "# 30 total minutes processing time: 15 minutes to process xml, \n",
    "# 6   minutes for each aggegration step, 2 minutes for the column summary\n",
    "# Final output: 60MB of 'ALL_' Excel 29 files (largest: ALL_quiz.xlsx 35MB, ALL_logstores 10MB, ALL_forum 5MB)\n",
    "# The initial section output (moved to _EACH_SECTION_/) has 334 xlsx files, \n",
    "# which is futher reduced (see _ALL_SECTIONS_ ) 67 files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not archive_source_file and not expanded_archive_directory:\n",
    "    raise ValueError('No mbz archive file or archive directory (with .xml files) is set')\n",
    "\n",
    "if archive_source_file and not os.path.isfile(archive_source_file) :\n",
    "    raise ValueError('archive_source_file (' + os.path.abspath(archive_source_file) + \") does not refer to an existing archive\")\n",
    "\n",
    "if not expanded_archive_directory:\n",
    "    expanded_archive_directory = archive_file_to_xml_dir(archive_source_file)\n",
    "\n",
    "if not output_directory:\n",
    "    if archive_source_file:\n",
    "        output_directory = archive_file_to_output_dir(archive_source_file)\n",
    "    else:\n",
    "        raise ValueError('Please specify output_directory')\n",
    "    \n",
    "if anonid_csv_file:\n",
    "    anonid_df = pd.read_csv(anonid_csv_file,header=True)\n",
    "else:\n",
    "    anonid_df = pd.DataFrame([{'userid':'-1','anonid':'example1234'}])\n",
    "\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(start_time)\n",
    "    \n",
    "if archive_source_file:\n",
    "    lazy_extract_mbz(archive_source_file,expanded_archive_directory,skip_expanding_if_xml_files_found)\n",
    "\n",
    "process_xml_files(expanded_archive_directory,output_directory)\n",
    "aggreate_over_sections(output_directory)\n",
    "aggreate_over_common_objects(output_directory)\n",
    "create_column_metalist(output_directory)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(end_time)\n",
    "print(end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
